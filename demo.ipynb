{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How It Works\n",
    "\n",
    "   ## Video Processing:\n",
    "\n",
    "   The video is opened using cv2.VideoCapture.\n",
    "\n",
    "   Each frame is processed to detect the cat face and extract features.\n",
    "\n",
    "   ## Feature Extraction:\n",
    "\n",
    "   The extract_features function extracts HSV and LBP features from the detected cat face.\n",
    "\n",
    "   ## Prediction:\n",
    "\n",
    "   The trained model predicts the cat label for each frame.\n",
    "\n",
    "   ## Majority Voting:\n",
    "\n",
    "   The predictions for all frames are counted, and the majority label is determined using Counter.\n",
    "\n",
    "   ## Output:\n",
    "\n",
    "   The majority label is printed as the final classification for the video."
   ],
   "id": "2d13aa3cc647b853"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:05:24.416107Z",
     "start_time": "2025-03-13T16:04:14.924376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage.feature import local_binary_pattern\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration Constants\n",
    "HAAR_CASCADE_PATH = 'haarcascade_frontalcatface.xml'\n",
    "GRID_SIZE = (4, 4)  # Split face into 4x4 grid\n",
    "FACE_SIZE = (100, 100)  # Standard size for face resizing\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('cat_classifier.pkl')\n",
    "\n",
    "# Load the Haar Cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "if face_cascade.empty():\n",
    "    raise FileNotFoundError(\"Could not load Haar Cascade file. Check the path.\")\n",
    "\n",
    "\n",
    "def extract_features(frame):\n",
    "    \"\"\"\n",
    "    Extract distinguishing features from a frame containing a cat face\n",
    "    Returns feature vector or None if no face detected\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect cat faces using Haar Cascade\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take largest face (assuming single cat per frame)\n",
    "    (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])\n",
    "    face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "    # Resize face to standard size for consistent feature extraction\n",
    "    face_roi = cv2.resize(face_roi, FACE_SIZE)\n",
    "\n",
    "    # --- Color Features (HSV Space) ---\n",
    "    hsv = cv2.cvtColor(face_roi, cv2.COLOR_BGR2HSV)\n",
    "    cell_height = FACE_SIZE[0] // GRID_SIZE[0]\n",
    "    cell_width = FACE_SIZE[1] // GRID_SIZE[1]\n",
    "\n",
    "    hsv_features = []\n",
    "    for i in range(GRID_SIZE[0]):\n",
    "        for j in range(GRID_SIZE[1]):\n",
    "            # Extract grid cell\n",
    "            y_start = i * cell_height\n",
    "            y_end = (i + 1) * cell_height\n",
    "            x_start = j * cell_width\n",
    "            x_end = (j + 1) * cell_width\n",
    "\n",
    "            cell = hsv[y_start:y_end, x_start:x_end]\n",
    "            # Calculate mean HSV values for the cell\n",
    "            hsv_features.extend(np.mean(cell, axis=(0, 1)))\n",
    "\n",
    "    # --- Texture Features (LBP) ---\n",
    "    gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(\n",
    "        gray_face,\n",
    "        P=8,  # 8 surrounding points\n",
    "        R=1,  # Radius of 1 pixel\n",
    "        method='uniform'\n",
    "    )\n",
    "    # Create histogram of LBP patterns (16 bins)\n",
    "    lbp_hist, _ = np.histogram(\n",
    "        lbp,\n",
    "        bins=16,\n",
    "        range=(0, 16)\n",
    "    )\n",
    "\n",
    "    # Combine all features into single array\n",
    "    return np.concatenate([hsv_features, lbp_hist])\n",
    "\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"demofootage.mov\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Process each frame of the video\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Extract features from the frame\n",
    "    features = extract_features(frame)\n",
    "\n",
    "    if features is not None:\n",
    "        # Predict the cat label using the trained model\n",
    "        prediction = model.predict([features])[0]\n",
    "        predictions.append(prediction)\n",
    "        frame_count += 1\n",
    "\n",
    "    # Optional: Display the frame with the predicted label (for debugging)\n",
    "    if features is not None:\n",
    "        cv2.putText(frame, f\"Predicted: {prediction}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit early\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Determine the majority prediction\n",
    "if predictions:\n",
    "    majority_label = Counter(predictions).most_common(1)[0][0]\n",
    "    print(f\"This is a video clip of {majority_label}.\")\n",
    "else:\n",
    "    print(\"No cat faces detected in the video.\")"
   ],
   "id": "3d47a12adf1b1382",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 102\u001B[39m\n\u001B[32m     99\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    101\u001B[39m \u001B[38;5;66;03m# Extract features from the frame\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m102\u001B[39m features = \u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    105\u001B[39m     \u001B[38;5;66;03m# Predict the cat label using the trained model\u001B[39;00m\n\u001B[32m    106\u001B[39m     prediction = model.predict([features])[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 30\u001B[39m, in \u001B[36mextract_features\u001B[39m\u001B[34m(frame)\u001B[39m\n\u001B[32m     27\u001B[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Detect cat faces using Haar Cascade\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m faces = \u001B[43mface_cascade\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdetectMultiScale\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscaleFactor\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43mminNeighbors\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43mminSize\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(faces) == \u001B[32m0\u001B[39m:\n\u001B[32m     38\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Press 'q' to quit early",
   "id": "6fbbec033690ae93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Local Binary Patterns (LBP)\n",
    "LBP captures the local texture information by comparing each pixel in an image with its neighboring pixels. It generates a binary code for each pixel based on whether its neighbors are brighter or darker than itself.\n",
    "\n",
    "## Steps to Compute LBP\n",
    "### Step 1: Define a Neighborhood\n",
    "\n",
    "For each pixel in the image, consider a small neighborhood around it (e.g., a 3x3 grid).\n",
    "\n",
    "The center pixel is the one being analyzed, and the 8 surrounding pixels are its neighbors.\n",
    "\n",
    "### Step 2: Compare Neighbors to the Center Pixel\n",
    "\n",
    "Compare the intensity value of each neighbor to the intensity value of the center pixel.\n",
    "\n",
    "If the neighbor's intensity is greater than or equal to the center pixel's intensity, assign a 1 to that neighbor.\n",
    "\n",
    "If the neighbor's intensity is less than the center pixel's intensity, assign a 0.\n",
    "\n",
    "### Step 3: Generate the Binary Pattern\n",
    "\n",
    " Starting from a fixed position (e.g., top-left neighbor), collect the binary values (0s and 1s) from all neighbors in a clockwise or counterclockwise order.\n",
    "\n",
    " Convert this binary sequence into a decimal number. This decimal number is the LBP code for the center pixel.\n",
    "\n",
    "### Step 4: Repeat for All Pixels\n",
    "\n",
    "Repeat the above steps for every pixel in the image to generate an LBP-encoded image.\n",
    "\n"
   ],
   "id": "9ba098ae4c374820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage.feature import local_binary_pattern\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration Constants\n",
    "HAAR_CASCADE_PATH = 'haarcascade_frontalcatface.xml'\n",
    "GRID_SIZE = (4, 4)  # Split face into 4x4 grid\n",
    "FACE_SIZE = (100, 100)  # Standard size for face resizing\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('cat_classifier.pkl')\n",
    "\n",
    "# Load the Haar Cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "if face_cascade.empty():\n",
    "    raise FileNotFoundError(\"Could not load Haar Cascade file. Check the path.\")\n",
    "\n",
    "\n",
    "def extract_features(frame):\n",
    "    \"\"\"\n",
    "    Extract distinguishing features from a frame containing a cat face\n",
    "    Returns feature vector or None if no face detected\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect cat faces using Haar Cascade\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take largest face (assuming single cat per frame)\n",
    "    (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])\n",
    "    face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "    # Resize face to standard size for consistent feature extraction\n",
    "    face_roi = cv2.resize(face_roi, FACE_SIZE)\n",
    "\n",
    "    # --- Color Features (HSV Space) ---\n",
    "    hsv = cv2.cvtColor(face_roi, cv2.COLOR_BGR2HSV)\n",
    "    cell_height = FACE_SIZE[0] // GRID_SIZE[0]\n",
    "    cell_width = FACE_SIZE[1] // GRID_SIZE[1]\n",
    "\n",
    "    hsv_features = []\n",
    "    for i in range(GRID_SIZE[0]):\n",
    "        for j in range(GRID_SIZE[1]):\n",
    "            # Extract grid cell\n",
    "            y_start = i * cell_height\n",
    "            y_end = (i + 1) * cell_height\n",
    "            x_start = j * cell_width\n",
    "            x_end = (j + 1) * cell_width\n",
    "\n",
    "            cell = hsv[y_start:y_end, x_start:x_end]\n",
    "            # Calculate mean HSV values for the cell\n",
    "            hsv_features.extend(np.mean(cell, axis=(0, 1)))\n",
    "\n",
    "    # --- Texture Features (LBP) ---\n",
    "    gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(\n",
    "        gray_face,\n",
    "        P=8,  # 8 surrounding points\n",
    "        R=1,  # Radius of 1 pixel\n",
    "        method='uniform'\n",
    "    )\n",
    "    # Create histogram of LBP patterns (16 bins)\n",
    "    lbp_hist, _ = np.histogram(\n",
    "        lbp,\n",
    "        bins=16,\n",
    "        range=(0, 16)\n",
    "    )\n",
    "\n",
    "    # Combine all features into single array\n",
    "    return np.concatenate([hsv_features, lbp_hist])\n",
    "\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"demofootage2.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Process each frame of the video\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Extract features from the frame\n",
    "    features = extract_features(frame)\n",
    "\n",
    "    if features is not None:\n",
    "        # Predict the cat label using the trained model\n",
    "        prediction = model.predict([features])[0]\n",
    "        predictions.append(prediction)\n",
    "        frame_count += 1\n",
    "\n",
    "    # Optional: Display the frame with the predicted label (for debugging)\n",
    "    if features is not None:\n",
    "        cv2.putText(frame, f\"Predicted: {prediction}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit early\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Determine the majority prediction\n",
    "if predictions:\n",
    "    majority_label = Counter(predictions).most_common(1)[0][0]\n",
    "    print(f\"This is a video clip of {majority_label}.\")\n",
    "else:\n",
    "    print(\"No cat faces detected in the video.\")"
   ],
   "id": "694071bfffeafbc9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hue, Saturation, Value (HSV)\n",
    "is a color space that separates color information into three distinct components, making it more intuitive and useful for certain image processing tasks compared to the RGB color space. Here's a detailed explanation of how HSV features work:\n",
    "## 1. What is HSV?\n",
    "\n",
    "HSV is an alternative representation of the RGB color space. It separates color information into:\n",
    "\n",
    "Hue (H): Represents the dominant wavelength of the color (e.g., red, green, blue).\n",
    "\n",
    "Saturation (S): Represents the purity or intensity of the color (e.g., pastel vs. vivid).\n",
    "\n",
    "Value (V): Represents the brightness of the color (e.g., dark vs. bright).\n",
    "\n",
    "## 2. Why Use HSV?\n",
    "\n",
    "Intuitive: Separates color (Hue) from brightness (Value) and color intensity (Saturation).\n",
    "\n",
    "Robustness: Less sensitive to lighting variations compared to RGB.\n",
    "\n",
    "Useful for Segmentation: Easier to isolate specific colors or ranges of colors."
   ],
   "id": "9caa70cc0957125b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "from skimage.feature import local_binary_pattern\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration Constants\n",
    "HAAR_CASCADE_PATH = 'haarcascade_frontalcatface.xml'\n",
    "GRID_SIZE = (4, 4)  # Split face into 4x4 grid\n",
    "FACE_SIZE = (100, 100)  # Standard size for face resizing\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('cat_classifier.pkl')\n",
    "\n",
    "# Load the Haar Cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)\n",
    "if face_cascade.empty():\n",
    "    raise FileNotFoundError(\"Could not load Haar Cascade file. Check the path.\")\n",
    "\n",
    "\n",
    "def extract_features(frame):\n",
    "    \"\"\"\n",
    "    Extract distinguishing features from a frame containing a cat face\n",
    "    Returns feature vector or None if no face detected\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect cat faces using Haar Cascade\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "\n",
    "    # Take largest face (assuming single cat per frame)\n",
    "    (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])\n",
    "    face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "    # Resize face to standard size for consistent feature extraction\n",
    "    face_roi = cv2.resize(face_roi, FACE_SIZE)\n",
    "\n",
    "    # --- Color Features (HSV Space) ---\n",
    "    hsv = cv2.cvtColor(face_roi, cv2.COLOR_BGR2HSV)\n",
    "    cell_height = FACE_SIZE[0] // GRID_SIZE[0]\n",
    "    cell_width = FACE_SIZE[1] // GRID_SIZE[1]\n",
    "\n",
    "    hsv_features = []\n",
    "    for i in range(GRID_SIZE[0]):\n",
    "        for j in range(GRID_SIZE[1]):\n",
    "            # Extract grid cell\n",
    "            y_start = i * cell_height\n",
    "            y_end = (i + 1) * cell_height\n",
    "            x_start = j * cell_width\n",
    "            x_end = (j + 1) * cell_width\n",
    "\n",
    "            cell = hsv[y_start:y_end, x_start:x_end]\n",
    "            # Calculate mean HSV values for the cell\n",
    "            hsv_features.extend(np.mean(cell, axis=(0, 1)))\n",
    "\n",
    "    # --- Texture Features (LBP) ---\n",
    "    gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = local_binary_pattern(\n",
    "        gray_face,\n",
    "        P=8,  # 8 surrounding points\n",
    "        R=1,  # Radius of 1 pixel\n",
    "        method='uniform'\n",
    "    )\n",
    "    # Create histogram of LBP patterns (16 bins)\n",
    "    lbp_hist, _ = np.histogram(\n",
    "        lbp,\n",
    "        bins=16,\n",
    "        range=(0, 16)\n",
    "    )\n",
    "\n",
    "    # Combine all features into single array\n",
    "    return np.concatenate([hsv_features, lbp_hist])\n",
    "\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"demofootage3.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    raise FileNotFoundError(f\"Could not open video file: {video_path}\")\n",
    "\n",
    "# Initialize a list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Process each frame of the video\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Extract features from the frame\n",
    "    features = extract_features(frame)\n",
    "\n",
    "    if features is not None:\n",
    "        # Predict the cat label using the trained model\n",
    "        prediction = model.predict([features])[0]\n",
    "        predictions.append(prediction)\n",
    "        frame_count += 1\n",
    "\n",
    "    # Optional: Display the frame with the predicted label (for debugging)\n",
    "    if features is not None:\n",
    "        cv2.putText(frame, f\"Predicted: {prediction}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit early\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Determine the majority prediction\n",
    "if predictions:\n",
    "    majority_label = Counter(predictions).most_common(1)[0][0]\n",
    "    print(f\"This is a video clip of {majority_label}.\")\n",
    "else:\n",
    "    print(\"No cat faces detected in the video.\")"
   ],
   "id": "9bc741b0394671bf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
